{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('got.txt','r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Game Of Thrones \\nBook One of A Song of Ice and Fire \\nBy George R. R. Martin \\nPROLOGUE \\n\"We should start back,\" Gared urged as the woods began to grow dark around them. \"The wildlings are \\ndead.\" \\n\"Do the dead frighten you?\" Ser Waymar Royce asked with just the hint of a smile. \\nGared did not rise to the bait. He was an old man, past fifty, and he had seen the lordlings come and go. \\n\"Dead is dead,\" he said. \"We have no business with the dead.\" \\n\"Are they dead?\" Royce asked softly. \"What proof have we?\" \\n\"Will saw them,\" Gared said. \"If he says they are dead, that\\'s proof enough for me.\" \\nWill had known they would drag him into the quarrel sooner or later. He wished it had been later rather \\nthan sooner. \"My mother told me that dead men sing no songs,\" he put in. \\n\"My wet nurse said the same thing, Will,\" Royce replied. \"Never believe anything you hear at a woman\\'s \\ntit. There are things to be learned even from the dead.\" His voice echoed, too loud in the twilit forest. \\nPage 1\\n\\n\"We h'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique characters in the text\n",
    "chars = tuple(set(text))\n",
    "\n",
    "# Dictionaries from char to index and index to char\n",
    "idx2char = dict(enumerate(chars))\n",
    "char2idx = {ch: i for i,ch in idx2char.items()}\n",
    "\n",
    "# According to the dictionary, convert the characters in the book to integers\n",
    "encoded = np.array([char2idx[ch] for ch in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  1, 63, 74,  6, 70,  1, 64, 10,  1, 34,  0, 54, 30, 28, 70, 58,\n",
       "        1, 16, 47, 30, 30, 43,  1, 64, 28, 70,  1, 30, 10,  1,  3,  1, 27,\n",
       "       30, 28, 23,  1, 30, 10,  1, 75, 56, 70,  1, 74, 28, 11,  1, 52, 38,\n",
       "       54, 70,  1, 16, 47, 62,  1, 63, 70, 30, 54, 23, 70,  1, 46, 49,  1,\n",
       "       46, 49,  1, 32, 74, 54, 61, 38, 28,  1, 16, 44, 46, 64, 42, 64, 63,\n",
       "       71, 53,  1, 16,  4, 15, 70,  1, 58,  0, 30, 51, 14, 11,  1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apparently. As you can see in our char-RNN image above, our LSTM expects an input that is one-hot encoded meaning that \n",
    "# each character is converted into an integer (via our created dictionary) and then converted into a column vector where \n",
    "# only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since \n",
    "# we're one-hot encoding the data, let's make a function to do that!\n",
    "\n",
    "def one_hot_encoder(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(np.array([3,5,1]),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(arr, batch_size, seq_length):\n",
    "    # No. of characters in a batch = batch_size * seq_length\n",
    "    # Therefore no. of batches => \n",
    "    \n",
    "    k = arr.size//(batch_size * seq_length) # No. of batches\n",
    "    arr = arr[0:(batch_size * seq_length * k)]\n",
    "    \n",
    "    # Reshape array into arrays of size length batch_size\n",
    "    arr = arr.reshape(batch_size,-1)\n",
    "    \n",
    "    # Now we iterate over the array all at once with a window of batch_size x seq_length, then advance \n",
    "    # by seq_length\n",
    "    \n",
    "    for n in range(0,arr.shape[1],seq_length):\n",
    "        x = arr[:,n:(n+seq_length)]\n",
    "        \n",
    "        # The targets are x shifted by one (cause we're predicting the next character)\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batch_generator(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 3  1 63 74  6 70  1 64 10  1]\n",
      " [15  0 70 28  1  0 70  1 16 54]\n",
      " [16 32 30 54  6 30 28 61  1 67]\n",
      " [ 0 70  1 43 38 28 23 17  4  1]\n",
      " [ 1 29 70 54  1 70 62 70 58  1]\n",
      " [67  0 70 28 70 19 70 54  1 16]\n",
      " [28 49  4  1 16  4  9 30 51 54]\n",
      " [54 74  0  1 10 54 30 67 28 70]]\n",
      "\n",
      "y\n",
      " [[ 1 63 74  6 70  1 64 10  1 34]\n",
      " [ 0 70 28  1  0 70  1 16 54 70]\n",
      " [32 30 54  6 30 28 61  1 67 74]\n",
      " [70  1 43 38 28 23 17  4  1 72]\n",
      " [29 70 54  1 70 62 70 58  1 67]\n",
      " [ 0 70 28 70 19 70 54  1 16  0]\n",
      " [49  4  1 16  4  9 30 51 54  1]\n",
      " [74  0  1 10 54 30 67 28 70 11]]\n"
     ]
    }
   ],
   "source": [
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:, :10])\n",
    "print('\\ny\\n', y[:, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GotRNN(nn.Module):\n",
    "    def __init__(self, chars, hidden_size, n_layers, drop_prob = 0.8, lr = 0.001):\n",
    "        \n",
    "        super(GotRNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Creating dictionaries\n",
    "        self.chars = chars\n",
    "        self.idx2char = dict(enumerate(self.chars))\n",
    "        self.char2idx = {ch: i for i,ch in self.idx2char.items()}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars), hidden_size, n_layers, dropout = drop_prob, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, len(self.chars)) # Output size is also the length of the text, just one forwarded.\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        out = out.contiguous().view(-1, self.hidden_size)\n",
    "\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_size,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_size).zero_())\n",
    "        \n",
    "        return hidden  \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GotRNN(\n",
      "  (lstm): LSTM(78, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc): Linear(in_features=512, out_features=78, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "seq_length = 64\n",
    "lr = 0.001\n",
    "clip = 5\n",
    "val_frac = 0.2\n",
    "print_every = 64\n",
    "drop_prob = 0.5\n",
    "\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "net = GotRNN(chars, hidden_size, n_layers, drop_prob=drop_prob)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs, batch_size, seq_length, lr, clip, val_frac, print_every):\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # Validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in batch_generator(data, batch_size, seq_length):\n",
    "            \n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encoder(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Create new variable for the hidden state\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # Zero accumulated grads\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in batch_generator(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encoder(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "            \n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 64... Loss: 3.0895... Val Loss: 3.0825\n",
      "Epoch: 1/20... Step: 128... Loss: 2.8238... Val Loss: 2.7622\n",
      "Epoch: 1/20... Step: 192... Loss: 2.4209... Val Loss: 2.3484\n",
      "Epoch: 1/20... Step: 256... Loss: 2.2366... Val Loss: 2.1980\n",
      "Epoch: 2/20... Step: 320... Loss: 2.1264... Val Loss: 2.0852\n",
      "Epoch: 2/20... Step: 384... Loss: 2.0582... Val Loss: 1.9988\n",
      "Epoch: 2/20... Step: 448... Loss: 1.9383... Val Loss: 1.9254\n",
      "Epoch: 2/20... Step: 512... Loss: 1.9479... Val Loss: 1.8648\n",
      "Epoch: 2/20... Step: 576... Loss: 1.8944... Val Loss: 1.8153\n",
      "Epoch: 3/20... Step: 640... Loss: 1.8932... Val Loss: 1.7597\n",
      "Epoch: 3/20... Step: 704... Loss: 1.7709... Val Loss: 1.7164\n",
      "Epoch: 3/20... Step: 768... Loss: 1.7507... Val Loss: 1.6811\n",
      "Epoch: 3/20... Step: 832... Loss: 1.6929... Val Loss: 1.6443\n",
      "Epoch: 3/20... Step: 896... Loss: 1.7242... Val Loss: 1.6147\n",
      "Epoch: 4/20... Step: 960... Loss: 1.6761... Val Loss: 1.5867\n",
      "Epoch: 4/20... Step: 1024... Loss: 1.6239... Val Loss: 1.5659\n",
      "Epoch: 4/20... Step: 1088... Loss: 1.6379... Val Loss: 1.5433\n",
      "Epoch: 4/20... Step: 1152... Loss: 1.6077... Val Loss: 1.5218\n",
      "Epoch: 4/20... Step: 1216... Loss: 1.5897... Val Loss: 1.5054\n",
      "Epoch: 5/20... Step: 1280... Loss: 1.5496... Val Loss: 1.4871\n",
      "Epoch: 5/20... Step: 1344... Loss: 1.5091... Val Loss: 1.4744\n",
      "Epoch: 5/20... Step: 1408... Loss: 1.5363... Val Loss: 1.4623\n",
      "Epoch: 5/20... Step: 1472... Loss: 1.5066... Val Loss: 1.4500\n",
      "Epoch: 5/20... Step: 1536... Loss: 1.4673... Val Loss: 1.4373\n",
      "Epoch: 6/20... Step: 1600... Loss: 1.5082... Val Loss: 1.4243\n",
      "Epoch: 6/20... Step: 1664... Loss: 1.4442... Val Loss: 1.4176\n",
      "Epoch: 6/20... Step: 1728... Loss: 1.4567... Val Loss: 1.4071\n",
      "Epoch: 6/20... Step: 1792... Loss: 1.4710... Val Loss: 1.3987\n",
      "Epoch: 6/20... Step: 1856... Loss: 1.4487... Val Loss: 1.3926\n",
      "Epoch: 7/20... Step: 1920... Loss: 1.4478... Val Loss: 1.3869\n",
      "Epoch: 7/20... Step: 1984... Loss: 1.3799... Val Loss: 1.3777\n",
      "Epoch: 7/20... Step: 2048... Loss: 1.4118... Val Loss: 1.3718\n",
      "Epoch: 7/20... Step: 2112... Loss: 1.4590... Val Loss: 1.3669\n",
      "Epoch: 7/20... Step: 2176... Loss: 1.3305... Val Loss: 1.3593\n",
      "Epoch: 8/20... Step: 2240... Loss: 1.4250... Val Loss: 1.3556\n",
      "Epoch: 8/20... Step: 2304... Loss: 1.4087... Val Loss: 1.3482\n",
      "Epoch: 8/20... Step: 2368... Loss: 1.4096... Val Loss: 1.3425\n",
      "Epoch: 8/20... Step: 2432... Loss: 1.3918... Val Loss: 1.3371\n",
      "Epoch: 8/20... Step: 2496... Loss: 1.3528... Val Loss: 1.3329\n",
      "Epoch: 9/20... Step: 2560... Loss: 1.3096... Val Loss: 1.3292\n",
      "Epoch: 9/20... Step: 2624... Loss: 1.3357... Val Loss: 1.3272\n",
      "Epoch: 9/20... Step: 2688... Loss: 1.3596... Val Loss: 1.3226\n",
      "Epoch: 9/20... Step: 2752... Loss: 1.3656... Val Loss: 1.3222\n",
      "Epoch: 9/20... Step: 2816... Loss: 1.3590... Val Loss: 1.3165\n",
      "Epoch: 10/20... Step: 2880... Loss: 1.3040... Val Loss: 1.3103\n",
      "Epoch: 10/20... Step: 2944... Loss: 1.3100... Val Loss: 1.3078\n",
      "Epoch: 10/20... Step: 3008... Loss: 1.3200... Val Loss: 1.3058\n",
      "Epoch: 10/20... Step: 3072... Loss: 1.3180... Val Loss: 1.3021\n",
      "Epoch: 10/20... Step: 3136... Loss: 1.3162... Val Loss: 1.2985\n",
      "Epoch: 11/20... Step: 3200... Loss: 1.2964... Val Loss: 1.3002\n",
      "Epoch: 11/20... Step: 3264... Loss: 1.2885... Val Loss: 1.2946\n",
      "Epoch: 11/20... Step: 3328... Loss: 1.3083... Val Loss: 1.2915\n",
      "Epoch: 11/20... Step: 3392... Loss: 1.3556... Val Loss: 1.2898\n",
      "Epoch: 12/20... Step: 3456... Loss: 1.3064... Val Loss: 1.2847\n",
      "Epoch: 12/20... Step: 3520... Loss: 1.2928... Val Loss: 1.2842\n",
      "Epoch: 12/20... Step: 3584... Loss: 1.2882... Val Loss: 1.2829\n",
      "Epoch: 12/20... Step: 3648... Loss: 1.3445... Val Loss: 1.2816\n",
      "Epoch: 12/20... Step: 3712... Loss: 1.2738... Val Loss: 1.2793\n",
      "Epoch: 13/20... Step: 3776... Loss: 1.2666... Val Loss: 1.2772\n",
      "Epoch: 13/20... Step: 3840... Loss: 1.2564... Val Loss: 1.2744\n",
      "Epoch: 13/20... Step: 3904... Loss: 1.2205... Val Loss: 1.2760\n",
      "Epoch: 13/20... Step: 3968... Loss: 1.2765... Val Loss: 1.2694\n",
      "Epoch: 13/20... Step: 4032... Loss: 1.2631... Val Loss: 1.2671\n",
      "Epoch: 14/20... Step: 4096... Loss: 1.3195... Val Loss: 1.2659\n",
      "Epoch: 14/20... Step: 4160... Loss: 1.2787... Val Loss: 1.2646\n",
      "Epoch: 14/20... Step: 4224... Loss: 1.2563... Val Loss: 1.2655\n",
      "Epoch: 14/20... Step: 4288... Loss: 1.2616... Val Loss: 1.2656\n",
      "Epoch: 14/20... Step: 4352... Loss: 1.3055... Val Loss: 1.2602\n",
      "Epoch: 15/20... Step: 4416... Loss: 1.2359... Val Loss: 1.2594\n",
      "Epoch: 15/20... Step: 4480... Loss: 1.2836... Val Loss: 1.2607\n",
      "Epoch: 15/20... Step: 4544... Loss: 1.2112... Val Loss: 1.2586\n",
      "Epoch: 15/20... Step: 4608... Loss: 1.2815... Val Loss: 1.2551\n",
      "Epoch: 15/20... Step: 4672... Loss: 1.2552... Val Loss: 1.2529\n",
      "Epoch: 16/20... Step: 4736... Loss: 1.2442... Val Loss: 1.2552\n",
      "Epoch: 16/20... Step: 4800... Loss: 1.2473... Val Loss: 1.2554\n",
      "Epoch: 16/20... Step: 4864... Loss: 1.2769... Val Loss: 1.2535\n",
      "Epoch: 16/20... Step: 4928... Loss: 1.2466... Val Loss: 1.2504\n",
      "Epoch: 16/20... Step: 4992... Loss: 1.2217... Val Loss: 1.2489\n",
      "Epoch: 17/20... Step: 5056... Loss: 1.2654... Val Loss: 1.2487\n",
      "Epoch: 17/20... Step: 5120... Loss: 1.1717... Val Loss: 1.2487\n",
      "Epoch: 17/20... Step: 5184... Loss: 1.2052... Val Loss: 1.2486\n",
      "Epoch: 17/20... Step: 5248... Loss: 1.2135... Val Loss: 1.2452\n",
      "Epoch: 17/20... Step: 5312... Loss: 1.2211... Val Loss: 1.2464\n",
      "Epoch: 18/20... Step: 5376... Loss: 1.2193... Val Loss: 1.2453\n",
      "Epoch: 18/20... Step: 5440... Loss: 1.2008... Val Loss: 1.2474\n",
      "Epoch: 18/20... Step: 5504... Loss: 1.2209... Val Loss: 1.2459\n",
      "Epoch: 18/20... Step: 5568... Loss: 1.2180... Val Loss: 1.2446\n",
      "Epoch: 18/20... Step: 5632... Loss: 1.2194... Val Loss: 1.2415\n",
      "Epoch: 19/20... Step: 5696... Loss: 1.2414... Val Loss: 1.2389\n",
      "Epoch: 19/20... Step: 5760... Loss: 1.2130... Val Loss: 1.2417\n",
      "Epoch: 19/20... Step: 5824... Loss: 1.2233... Val Loss: 1.2379\n",
      "Epoch: 19/20... Step: 5888... Loss: 1.1997... Val Loss: 1.2406\n",
      "Epoch: 19/20... Step: 5952... Loss: 1.2102... Val Loss: 1.2379\n",
      "Epoch: 20/20... Step: 6016... Loss: 1.1659... Val Loss: 1.2349\n",
      "Epoch: 20/20... Step: 6080... Loss: 1.2040... Val Loss: 1.2361\n",
      "Epoch: 20/20... Step: 6144... Loss: 1.1690... Val Loss: 1.2362\n",
      "Epoch: 20/20... Step: 6208... Loss: 1.1779... Val Loss: 1.2369\n",
      "Epoch: 20/20... Step: 6272... Loss: 1.2009... Val Loss: 1.2356\n"
     ]
    }
   ],
   "source": [
    "train(net, encoded, epochs, batch_size, seq_length, lr, clip, val_frac, print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2idx[char]]])\n",
    "        x = one_hot_encoder(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.idx2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a shoulder of the stabling blade of his sister's bed the boy with a black brother of the Kingsguard. The steward would not have been the boy when he had never seen him a second, and the word was a strength of the \n",
      "silver on the story, and her father was a stallion, a boy with the bastard of the Kingsguard. \"It's not that the man's bear, the boy was the second way at her from the stables.\" \n",
      "\"I wonder,\" she said, \"but the man is too stand and still here.\" Her brother was almost as the stone stars were to start to him. \"The gods were there and so much to see.\" \n",
      "\"There's not the boy,\" she told him.\" \n",
      "\"The gods are all the south, the words will be a single of your sister's stale.\" \n",
      "\"I don't want to say, I'm to start to the \n",
      "boy and the \n",
      "bastard with a silver tower to see you,\" she said, her fingers and shouted to the stables. \n",
      "\"The king was all a start to step and see that the stars,\" she said, \"the boy will never be a silver than you and the \n",
      "seal was all, a man's brothers, and I'll \n"
     ]
    }
   ],
   "source": [
    "print(sample(net,1000,prime='The',top_k=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "model_name = 'GotRNN.net'\n",
    "\n",
    "checkpoint = {'hidden_size': net.hidden_size,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars,\n",
    "              'opt': opt.state_dict(),}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "with open('GotRNN.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = GotRNN(checkpoint['tokens'], hidden_size=checkpoint['hidden_size'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n",
    "opt.load_state_dict(checkpoint['opt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bastard,\" he told him. \"I will not be a longsword as they were to the stables.\" \n",
      "\"I will be a strong,\" said Ser Jorah Mormont and the \n",
      "book of her son. \"I have not been the bastards and the \n",
      "battlements of all of them, the bastard man to see the \n",
      "second woman the man's been a start of a boy. I have no company of the \n",
      "boy as the boy was the stables and stars and the sept of the seven seat, a moment the man was a stranger and start and too soon of her brother and the sept, and they wore the stallion of their \n",
      "silent stores, and her father was the stony stalls, and the septa to the \n",
      "stalls of the Wall, the \n",
      "bastard would be a single of the \n",
      "stables. They said, I have not brought your \n",
      "beautiful as the sept on the stalleon and the \n",
      "stone with a silver than the \n",
      "boy when he had told the stallion where he was the boy with the stableboys, and the stars were to see them to her. \n",
      "The septa would be the story of his sons, the sept of trees and start of the \n",
      "sides and the stone was a shadow of the silver of the stone of the stallion. \n",
      "The stewards were the stallion who had been a strength of the stableboys, and they would be a long way to her brother's bed to this throne to the \n",
      "stone of the stone, and they were to see her fingers. \n",
      "The man was still the stableboy and took their faces. \n",
      "That she was all the boy still were to see the stallion who was the boy, the way he was a boy of the silence of the \n",
      "basket. \n",
      "The waters were the bastard start was the stallion of the Trident, and the world was a boy to the sight of that stone and the \n",
      "bastard and the \n",
      "stables, and her fingers seemed to see his face and still have a more than to the \n",
      "boy and his son of House, and his sister's breath was to stand at her side, the stories, and they'd seen your \n",
      "brother and the \n",
      "stalls are any one of the \n",
      "silver that the sept of men, and the wine that the man who was, and the stone was a stableboy and her face to send the stallion of \n",
      "his brother than he would be thinking to stand at him as they stood at \n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "# print(sample(loaded, 2000, top_k=2, prime=\"The Lord of the Seven Kingdoms and Protector\"))\n",
    "print(sample(loaded, 2000, top_k=2, prime=\"Bastard\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
